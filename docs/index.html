<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Extending Lasso Model Fitting to Big Data • biglasso</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="bootstrap-toc.css">
<script src="bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="pkgdown.css" rel="stylesheet">
<script src="pkgdown.js"></script><meta property="og:title" content="Extending Lasso Model Fitting to Big Data">
<meta property="og:description" content="Extend lasso and elastic-net model fitting for ultrahigh-dimensional, 
    multi-gigabyte data sets that cannot be loaded into memory. Its much more 
    memory- and computation-efficient as compared to existing lasso-fitting packages 
    like glmnet and ncvreg', thus allowing for very powerful big data analysis 
    even with an ordinary laptop.">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    <div class="container template-home">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="index.html">biglasso</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">1.4.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="index.html">
    <span class="fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="articles/biglasso.html">Get started</a>
</li>
<li>
  <a href="reference/index.html">Reference</a>
</li>
<li>
  <a href="news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/YaohuiZeng/biglasso/">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="contents col-md-9">
<div id="biglasso-extend-lasso-model-fitting-to-big-data-in-r" class="section level1">
<div class="page-header"><h1 class="hasAnchor">
<a href="#biglasso-extend-lasso-model-fitting-to-big-data-in-r" class="anchor"></a>biglasso: Extend Lasso Model Fitting to Big Data in R</h1></div>

<p><code>biglasso</code> extends lasso and elastic-net linear and logistic regression models for ultrahigh-dimensional, multi-gigabyte data sets that cannot be loaded into memory. It utilizes memory-mapped files to store the massive data on the disk and only read those into memory whenever necessary during model fitting. Moreover, some advanced feature screening rules are proposed and implemented to accelerate the model fitting. <strong>As a result, this package is much more memory- and computation-efficient and highly scalable as compared to existing lasso-fitting packages such as <a href="https://CRAN.R-project.org/package=glmnet">glmnet</a> and <a href="https://CRAN.R-project.org/package=ncvreg">ncvreg</a></strong>. Bechmarking experiments using both simulated and real data sets show that <code>biglasso</code> is not only 1.5x to 4x times faster than existing packages, but also at least 2x more memory-efficient. More importantly, to the best of our knowledge, <code>biglasso</code> is the first R package that enables users to fit lasso models with data sets that are larger than available RAM, thus allowing for powerful big data analysis on an ordinary laptop.</p>
<div id="news" class="section level2">
<h2 class="hasAnchor">
<a href="#news" class="anchor"></a>News:</h2>
<ul>
<li>This package on GitHub has been updated to Version 1.4-0. See details in NEWS.</li>
<li>This package was ranked top 3 for <a href="http://stat-computing.org/awards/jmc/">2017 ASA Chambers Statistical Software Award</a>.</li>
<li>The technical paper of this package was selected as a Winner of <a href="http://stat-computing.org/awards/student/winners.html">2017 ASA Student Paper Competiton from Section on Statistical Computing</a>.</li>
</ul>
</div>
<div id="documentation" class="section level2">
<h2 class="hasAnchor">
<a href="#documentation" class="anchor"></a>Documentation:</h2>
<ul>
<li>Here are the <a href="https://CRAN.R-project.org/package=biglasso/biglasso.pdf">R Reference manual</a> and <a href="https://CRAN.R-project.org/package=biglasso/vignettes/biglasso.pdf">User guide</a>
</li>
<li>Here are the technical papers of the package: i) <a href="https://arxiv.org/abs/1701.05936">The software paper</a>; and ii) <a href="https://arxiv.org/abs/1704.08742">the paper of hybrid safe-strong rules</a>
</li>
</ul>
</div>
<div id="features" class="section level2">
<h2 class="hasAnchor">
<a href="#features" class="anchor"></a>Features:</h2>
<ol>
<li>It utilizes memory-mapped files to store the massive data on the disk, only loading data into memory when necessary during model fitting. Consequently, it’s able to seamlessly handle out-of-core computation.</li>
<li>It is built upon pathwise coordinate descent algorithm with <em>warm start, active set cycling, and feature screening</em> strategies, which has been proven to be one of fastest lasso solvers.</li>
<li>We develop new, adaptive feature screening rules that outperform state-of-the-art screening rules such as the sequential strong rule (SSR) and the sequential EDPP rule (SEDPP) with additional 1.5x to 4x speedup.</li>
<li>The implementation is designed to be as memory-efficient as possible by eliminating extra copies of the data created by other R packages, making <code>biglasso</code> at least 2x more memory-efficient than <code>glmnet</code>.</li>
<li>The underlying computation is implemented in C++, and parallel computing with OpenMP is also supported.</li>
</ol>
</div>
<div id="benchmarks" class="section level2">
<h2 class="hasAnchor">
<a href="#benchmarks" class="anchor"></a>Benchmarks:</h2>
<div id="simulated-data" class="section level3">
<h3 class="hasAnchor">
<a href="#simulated-data" class="anchor"></a>Simulated data:</h3>
<ul>
<li>
<strong>Packages</strong> to be compared: <code><a href="reference/biglasso.html">biglasso (1.4-0)</a></code>, <code>glmnet (4.0-2)</code>, <code>ncvreg (3.12-0)</code>, and <code>picasso (1.3-1)</code>.</li>
<li>
<strong>Platform</strong>: AMD Ryzen 5 5600X @ 4.2 GHz and 32 GB RAM.</li>
<li>
<strong>Experiments</strong>: solving lasso-penalized linear regression over the entire path of 100 <code>lambda</code> values equally spaced on the log scale of <code>lambda / lambda_max</code> from 0.1 to 1; varying number of observations <code>n</code> and number of features <code>p</code>; 20 replications, the mean computing time (in seconds) are reported.</li>
<li>
<strong>Data generating model</strong>: <code>y =  X *  beta + 0.1 eps</code>, where <code>X</code> and <code>eps</code> are i.i.d. sampled from <code>N(0, 1)</code>.</li>
</ul>
<!--
![Alt text](/vignettes/2020-12-18_vary_p_pkgs.png?raw=true "Vary p")
![Alt text](/vignettes/2020-12-18_vary_n_pkgs.png?raw=true "Vary n")
--><div id="1-biglasso-is-more-computation-efficient" class="section level4">
<h4 class="hasAnchor">
<a href="#1-biglasso-is-more-computation-efficient" class="anchor"></a>(1) <code>biglasso</code> is more computation-efficient:</h4>
<!--
![Alt text](/vignettes/2020-12-18_vary_p_pkgs.png)
![Alt text](/vignettes/2020-12-18_vary_n_pkgs.png)
-->
<p><img src="https://raw.githubusercontent.com/YaohuiZeng/biglasso/master/vignettes/2020-12-18_vary_p_pkgs.png" width="400" height="300"><img src="https://raw.githubusercontent.com/YaohuiZeng/biglasso/master/vignettes/2020-12-18_vary_n_pkgs.png" width="400" height="300"></p>
<p>In all the settings, <code>biglasso</code> (1 core) is uniformly faster than <code>picasso</code>, <code>glmnet</code> and <code>ncvreg</code>. When the data gets bigger, <code>biglasso</code> achieves 6-9x speed-up compared to other packages. Moreover, the computing time of <code>biglasso</code> can be further reduced by half via parallel-computation of multiple cores.</p>
</div>
<div id="2-biglasso-is-more-memory-efficient" class="section level4">
<h4 class="hasAnchor">
<a href="#2-biglasso-is-more-memory-efficient" class="anchor"></a>(2) <code>biglasso</code> is more memory-efficient:</h4>
<p>To prove that <code>biglasso</code> is much more memory-efficient, we simulate a <code>1000 X 100000</code> large feature matrix. The raw data is 0.75 GB. We used <a href="https://github.com/jeetsukumaran/Syrupy">Syrupy</a> to measure the memory used in RAM (i.e. the resident set size, RSS) every 1 second during lasso model fitting by each of the packages.</p>
<p>The maximum RSS (in <strong>GB</strong>) used by a single fit and 10-fold cross validation is reported in the Table below. In the single fit case, <code>biglasso</code> consumes 0.60 GB memory in RAM, 23% of that used by <code>glmnet</code> and 24% of that used by <code>ncvreg</code>. Note that the memory consumed by <code>glmnet</code> and <code>ncvreg</code> are respectively 3.4x and 3.3x larger than the size of the raw data. <code>biglasso</code> also requires less additional memory to perform cross-validation, compared other packages. For serial 10-fold cross-validation, <code>biglasso</code> requires just 31% of the memory used by <code>glmnet</code> and 11% of that used by <code>ncvreg</code>, making it 3.2x and 9.4x more memory-efficient compared to these two, respectively.</p>
<center>
<table class="table">
<thead><tr class="header">
<th align="right">Package</th>
<th align="center">picasso</th>
<th align="center">ncvreg</th>
<th align="center">glmnet</th>
<th align="center">biglasso</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="right">Single fit</td>
<td align="center">0.74</td>
<td align="center">2.47</td>
<td align="center">2.57</td>
<td align="center">0.60</td>
</tr>
<tr class="even">
<td align="right">10-fold CV</td>
<td align="center">-</td>
<td align="center">4.62</td>
<td align="center">3.11</td>
<td align="center">0.96</td>
</tr>
</tbody>
</table>
</center>
<p><strong>Note</strong>: ..* the memory savings offered by <code>biglasso</code> would be even more significant if cross-validation were conducted in parallel. However, measuring memory usage across parallel processes is not straightforward and not implemented in <code>Syrupy</code>; ..* cross-validation is not implemented in <code>picasso</code> at this point.</p>
</div>
</div>
<div id="real-data" class="section level3">
<h3 class="hasAnchor">
<a href="#real-data" class="anchor"></a>Real data:</h3>
<p>The performance of the packages are also tested using diverse real data sets:</p>
<ul>
<li>
<a href="http://myweb.uiowa.edu/pbreheny/data/bcTCGA.html">Breast cancer gene expression data</a> (GENE);</li>
<li>
<a href="http://yann.lecun.com/exdb/mnist/">MNIST handwritten image data</a> (MNIST);</li>
<li>
<a href="https://arxiv.org/abs/1607.05636">Cardiac fibrosis genome-wide association study data</a> (GWAS);</li>
<li>
<a href="https://archive.ics.uci.edu/ml/datasets/Bag+of+Words">Subset of New York Times bag-of-words data</a> (NYT).</li>
</ul>
<p>The following table summarizes the mean (SE) computing time (in seconds) of solving the lasso along the entire path of 100 <code>lambda</code> values equally spaced on the log scale of <code>lambda / lambda_max</code> from 0.1 to 1 over 20 replications.</p>
<center>
<table class="table">
<thead><tr class="header">
<th align="right">Package</th>
<th align="center">GENE</th>
<th align="center">MNIST</th>
<th align="center">GWAS</th>
<th align="center">NYT</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="right"></td>
<td align="center"><code>n=536</code></td>
<td align="center"><code>n=784</code></td>
<td align="center"><code>n=313</code></td>
<td align="center"><code>n=5,000</code></td>
</tr>
<tr class="even">
<td align="right"></td>
<td align="center"><code>p=17,322</code></td>
<td align="center"><code>p=60,000</code></td>
<td align="center"><code>p=660,495</code></td>
<td align="center"><code>p=55,000</code></td>
</tr>
<tr class="odd">
<td align="right">picasso</td>
<td align="center">0.67 (0.02)</td>
<td align="center">2.94 (0.01)</td>
<td align="center">14.96 (0.01)</td>
<td align="center">15.91 (0.16)</td>
</tr>
<tr class="even">
<td align="right">ncvreg</td>
<td align="center">0.87 (0.01)</td>
<td align="center">4.22 (0.00)</td>
<td align="center">19.78 (0.01)</td>
<td align="center">25.59 (0.12)</td>
</tr>
<tr class="odd">
<td align="right">glmnet</td>
<td align="center">0.74 (0.01)</td>
<td align="center">3.82 (0.01)</td>
<td align="center">16.19 (0.01)</td>
<td align="center">24.94 (0.16)</td>
</tr>
<tr class="even">
<td align="right">biglasso</td>
<td align="center">0.31 (0.01)</td>
<td align="center">0.61 (0.02)</td>
<td align="center">4.82 (0.01)</td>
<td align="center">5.91 (0.78)</td>
</tr>
</tbody>
</table>
</center>
</div>
<div id="big-data-out-of-core-computation" class="section level3">
<h3 class="hasAnchor">
<a href="#big-data-out-of-core-computation" class="anchor"></a>Big data: Out-of-core computation</h3>
<p>To demonstrate the out-of-core computing capability of <code>biglasso</code>, a 96 GB real data set from a large-scale genome-wide association study is analyzed. The dimensionality of the design matrix is: <code>n = 973, p = 11,830,470</code>. <strong>Note that the size of data is 3x larger than the installed 32 GB of RAM.</strong></p>
<p>Since other three packages cannot handle this data-larger-than-RAM case, we compare the performance of screening rules <code>SSR</code> and <code>Adaptive</code> based on our package <code>biglasso</code>. In addition, two cases in terms of <code>lambda_min</code> are considered: (1) <code>lam_min = 0.1 lam_max</code>; and (2) <code>lam_min = 0.5 lam_max</code>, as in practice there is typically less interest in lower values of <code>lambda</code>for very high-dimensional data such as this case. Again the entire solution path with 100 <code>lambda</code> values is obtained. The table below summarizes the overall computing time (in <strong>minutes</strong>) by screening rule <code>SSR</code> (which is what other three packages are using) and our new rule <code>Adaptive</code>. (No replication is conducted.)</p>
<center>
<table class="table">
<thead><tr class="header">
<th align="left">Cases</th>
<th align="right">SSR</th>
<th align="right">Adaptive</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">
<code>lam_min / lam_max = 0.1</code>, 1 core</td>
<td align="right">189.67</td>
<td align="right">66.05</td>
</tr>
<tr class="even">
<td align="left">
<code>lam_min / lam_max = 0.1</code>, 4 cores</td>
<td align="right">86.31</td>
<td align="right">46.91</td>
</tr>
<tr class="odd">
<td align="left">
<code>lam_min / lam_max = 0.5</code>, 1 core</td>
<td align="right">177.84</td>
<td align="right">24.84</td>
</tr>
<tr class="even">
<td align="left">
<code>lam_min / lam_max = 0.5</code>, 4 cores</td>
<td align="right">85.67</td>
<td align="right">15.14</td>
</tr>
</tbody>
</table>
</center>
</div>
</div>
<div id="installation" class="section level2">
<h2 class="hasAnchor">
<a href="#installation" class="anchor"></a>Installation:</h2>
<ul>
<li>The stable version:</li>
</ul>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html">install.packages</a></span><span class="op">(</span><span class="st">"biglasso"</span><span class="op">)</span></code></pre></div>
<ul>
<li>The latest version:</li>
</ul>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">devtools</span><span class="fu">::</span><span class="fu"><a href="https://devtools.r-lib.org//reference/remote-reexports.html">install_github</a></span><span class="op">(</span><span class="st">"YaohuiZeng/biglasso"</span><span class="op">)</span></code></pre></div>
</div>
<div id="reference" class="section level2">
<h2 class="hasAnchor">
<a href="#reference" class="anchor"></a>Reference:</h2>
<ul>
<li>Zeng, Y., and Breheny, P. (2017). The biglasso Package: A Memory- and Computation-Efficient Solver for Lasso Model Fitting with Big Data in R. arXiv preprint arXiv:1701.05936. URL <a href="https://arxiv.org/abs/1701.05936">https://arxiv.org/abs/1701.05936</a>.</li>
<li>Tibshirani, R., Bien, J., Friedman, J., Hastie, T., Simon, N., Taylor, J., and Tibshirani, R. J. (2012). Strong rules for discarding predictors in lasso-type problems. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 74 (2), 245-266.</li>
<li>Wang, J., Zhou, J., Wonka, P., and Ye, J. (2013). Lasso screening rules via dual polytope projection. In Advances in Neural Information Processing Systems, pp. 1070-1078.</li>
<li>Xiang, Z. J., and Ramadge, P. J. (2012, March). Fast lasso screening tests based on correlations. In Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on (pp. 2137-2140). IEEE.</li>
<li>Wang, J., Zhou, J., Liu, J., Wonka, P., and Ye, J. (2014). A safe screening rule for sparse logistic regression. In Advances in Neural Information Processing Systems, pp. 1053-1061.</li>
</ul>
</div>
<div id="report-bugs" class="section level2">
<h2 class="hasAnchor">
<a href="#report-bugs" class="anchor"></a>Report bugs：</h2>
<ul>
<li>open an <a href="https://github.com/YaohuiZeng/biglasso/issues">issue</a> or send an email to Yaohui Zeng at <a href="mailto:yaohui.zeng@gmail.com" class="email">yaohui.zeng@gmail.com</a>
</li>
</ul>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">
    <div class="links">
<h2>Links</h2>
<ul class="list-unstyled">
<li>Download from CRAN at <br><a href="https://cloud.r-project.org/package=biglasso">https://​cloud.r-project.org/​package=biglasso</a>
</li>
<li>Browse source code at <br><a href="https://github.com/YaohuiZeng/biglasso/">https://​github.com/​YaohuiZeng/​biglasso/​</a>
</li>
<li>Report a bug at <br><a href="https://github.com/YaohuiZeng/biglasso/issues">https://​github.com/​YaohuiZeng/​biglasso/​issues</a>
</li>
</ul>
</div>
<div class="license">
<h2>License</h2>
<ul class="list-unstyled">
<li><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></li>
</ul>
</div>
<div class="citation">
<h2>Citation</h2>
<ul class="list-unstyled">
<li><a href="authors.html">Citing biglasso</a></li>
</ul>
</div>
<div class="developers">
<h2>Developers</h2>
<ul class="list-unstyled">
<li>Yaohui Zeng <br><small class="roles"> Maintainer </small>  </li>
</ul>
</div>

  <div class="dev-status">
<h2>Dev status</h2>
<ul class="list-unstyled">
<li><a href="https://travis-ci.org/YaohuiZeng/biglasso"><img src="https://travis-ci.org/YaohuiZeng/biglasso.svg?branch=master" alt="Build Status"></a></li>
<li><a href="https://CRAN.R-project.org/package=biglasso"><img src="http://www.r-pkg.org/badges/version/biglasso" alt="CRAN_Status_Badge"></a></li>
<li><a href="http://www.r-pkg.org/pkg/biglasso"><img src="http://cranlogs.r-pkg.org/badges/grand-total/biglasso" alt="CRAN RStudio mirror overall downloads"></a></li>
</ul>
</div>
</div>
</div>


      <footer><div class="copyright">
  <p>Developed by Yaohui Zeng.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.6.1.</p>
</div>

      </footer>
</div>

  


  </body>
</html>
